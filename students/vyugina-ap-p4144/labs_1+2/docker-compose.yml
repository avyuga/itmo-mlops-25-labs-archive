services:
  qdrant:
    image: qdrant/qdrant
    ports:
      - "${QDRANT_DASHBOARD_PORT}:6333"
    volumes:
      - qdrant-data:/qdrant/storage
  
  airflow-init:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile_AirFlow
    volumes:
      - ./mlops:/app/mlops
      - ./assets:/app/assets
      - airflow-data:/app/airflow
    environment:
      - AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW__CORE__DAGS_FOLDER}
      - PYTHONPATH=/app
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - DATA_DIR=${DATA_DIR}
    command: >
      bash -c "poetry run airflow db init &&
               poetry run airflow users create \
                --username admin \
                --firstname admin \
                --lastname admin \
                --role Admin \
                --email admin@example.com \
                --password admin || true"
    healthcheck:
      test: ["CMD", "poetry", "run", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$(hostname)"]
      interval: 10s
      timeout: 10s
      retries: 5
    depends_on:
      - qdrant

  airflow-webserver:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile_AirFlow
    ports:
      - "${AIRFLOW_WEBSERVER_HOST_PORT}:8080"
    volumes:
      - ./mlops:/app/mlops
      - ./assets:/app/assets
      - airflow-data:/app/airflow
    environment:
      - AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW__CORE__DAGS_FOLDER}
      - PYTHONPATH=/app
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - DATA_DIR=${DATA_DIR}
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      qdrant:
        condition: service_started

  airflow-scheduler:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile_AirFlow
    volumes:
      - ./mlops:/app/mlops
      - ./assets:/app/assets
      - airflow-data:/app/airflow
    environment:
      - AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW__CORE__DAGS_FOLDER}
      - PYTHONPATH=/app
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - DATA_DIR=${DATA_DIR}
    command: poetry run airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      qdrant:
        condition: service_started

  postgres:
    restart: always
    image: postgres
    expose:
      - "${PG_PORT}"
    networks:
      - backend
    environment:
      - POSTGRES_USER=${PG_USER}
      - POSTGRES_PASSWORD=${PG_PASSWORD}
      - POSTGRES_DATABASE=${PG_DATABASE}
    volumes:
      - postgres-data:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD", "pg_isready", "-p", "${PG_PORT}", "-U", "${PG_USER}"]
      interval: 5s
      timeout: 5s
      retries: 3

  minio:
    restart: always
    image: minio/minio
    container_name: minio-container
    volumes:
      - minio-data:/data
    ports:
      - "${MINIO_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    networks:
      - frontend
      - backend
      - minio_common
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_ADDRESS=${MINIO_ADDRESS}
      - MINIO_PORT=${MINIO_PORT}
      - MINIO_STORAGE_USE_HTTPS=${MINIO_STORAGE_USE_HTTPS}
      - MINIO_CONSOLE_ADDRESS=${MINIO_CONSOLE_ADDRESS}
    command: server /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio-create-bucket:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - minio_common
    entrypoint: >
      /bin/sh -c "
      mc alias set minio http://minio:9000/ ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      mc mb --ignore-existing minio/${MLFLOW_BUCKET_NAME};
      exit 0;
      "

  mlflow:
    restart: always
    build: 
      context: .
      dockerfile: dockerfiles/Dockerfile_MLFlow
    depends_on:
      - postgres
      - minio
    ports:
      - "${MLFLOW_PORT}:5000"
    networks:
      - frontend
      - backend
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_S3_ENDPOINT_URL=http://minio:${MINIO_PORT}
      - MLFLOW_S3_IGNORE_TLS=true
    command: >
      mlflow server
      --backend-store-uri postgresql://${PG_USER}:${PG_PASSWORD}@postgres:${PG_PORT}/${PG_DATABASE}
      --host 0.0.0.0
      --serve-artifacts
      --artifacts-destination s3://${MLFLOW_BUCKET_NAME}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MLFLOW_PORT}/"]
      interval: 30s
      timeout: 10s
      retries: 3


volumes:
  airflow-data:
  qdrant-data:
  postgres-data:
  minio-data:


networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
  minio_common: