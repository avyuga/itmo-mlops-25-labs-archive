# Лабораторная работа 1. Предобработка данных

1. Выбрать произвольный набор данных. В качестве источников может выступать:
    - Kaggle
    - Набор, используемый на хакатонах / домашних проектах
    - Самостоятельно собранные данные (через апи/парсер/что угодно)
    - ToDo: Добавить какой-нить дефолтный набор

2. Выполнить предварительную оценку решаемой проблемы:
    - Выполнить постановку задачи МО
    - Определить используемые метрики
    - Провести первичный анализ данных / работы с фичами:
        - обработать пустые значения
        - поискать корреляцию между признаками
        - выкинуть стоп слова или разбить на токены

3. Выбрать систему для хранения обработанных данных. Обосновать выбор:
    - S3: локальные или в облаке
    - Hadoop HDFS
    - Postgres для реляционных данных
    - Другие идеи

4. Выполнить разработанную в П2 предобработку:
    - Используя стек для больших данных (например, Spark) *
    - Либо используя стандартные средства (Pandas)

5. Выбрать систему оркестрации (например, AirFlow):
    - Подойдет любой способ развертывания
    - Выполнить автоматизацию выполнения пункта 4:
        - Добавить к имеющимся данным инкремент (например, загрузив отдельно самые последние данные в отдельный файл)
        - Или объединить с текущим*

## Полезные ресурсы
- Essential Guide to LLMOps, RYAN DOAN. Около 80 страниц с листингами кода, если отбросить довольно абстрактные штуки.
- Основные концепции AirFlow неплохо описаны в основной документации: https://airflow.apache.org/
- Если хочется копнуть в работу с AirFlow побольше: https://stepik.org/lesson/1410492/step/1?unit=1428000

### Spark ресурсы
- Обзорный курс на Scala (увы, платный): https://stepik.org/course/115252/syllabus
- В целом по Spark много всего можно найти.

\* - дополнительные баллы, но подумать
